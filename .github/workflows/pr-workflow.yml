name: pr-workflow

on:
  pull_request:

  # workflow_dispatch:
  #   inputs:
  #     remove_stack:
  #       description: 'Remove stack when finished?'
  #       required: true
  #       default: 'true'

jobs:
  build-us-west-2:
    runs-on: ubuntu-latest
    env:
      REGION: "us-west-2"
      VERSION: "3.0.0"
    #   EMAIL: ${{ secrets.INVITATION_EMAIL_RECIPIENT }}
    steps:
      - name: Check out pr branch
        uses: actions/checkout@v2
        with:
          ref: ${{ github.sha }}

      # - name: Initialize AWS credentials
      #   uses: aws-actions/configure-aws-credentials@v1
      #   with:
      #     aws-access-key-id: ${{ secrets.BUILD_AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.BUILD_AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Setup build environment
        run: |
          echo "SHORT_SHA=`git rev-parse --short HEAD`" >> $GITHUB_ENV
          DATETIME=$(date '+%s')
          echo "DIST_OUTPUT_BUCKET=solutions-reference" >> $GITHUB_ENV
          echo "TEMPLATE_OUTPUT_BUCKET=solutions-reference" >> $GITHUB_ENV

      - name: Run build script
        run: |
          cd deployment
          echo y | ./build-s3-dist.sh --template-bucket ${TEMPLATE_OUTPUT_BUCKET} --code-bucket ${DIST_OUTPUT_BUCKET} --version ${VERSION} --region ${REGION} | tee >( awk '/Without existing MIE deployment/{getline; print}' >template )

      # - name: Deploy stack
      #   run: |
      #     cd deployment
      #     TEMPLATE=$(cat template | cut -f 2 -d "'")
      #     rm -f template
      #     STACK_NAME="pr${SHORT_SHA}"
      #     set -x

      #     # Delete STACK_NAME if it already exists.
      #     # This is necessary in order to rerun github action workflows.
      #     # If $STACK_NAME exists...
      #     if [ $(echo $(aws cloudformation list-stacks --query 'StackSummaries[?StackName==`$STACK_NAME`]' --output text)  | tr -d '\n' | wc -c) > 0 ]; then
      #       # Then delete $STACK_NAME...
      #       echo "Removing $STACK_NAME so we can use that stack name again"
      #       aws cloudformation delete-stack --stack-name $STACK_NAME --region $REGION
      #       aws cloudformation wait stack-delete-complete --stack-name $STACK_NAME
      #     fi

      #     aws cloudformation create-stack --stack-name $STACK_NAME --template-url $TEMPLATE --region $REGION --parameters ParameterKey=AdminEmail,ParameterValue=$EMAIL --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM CAPABILITY_AUTO_EXPAND --disable-rollback
      #     aws cloudformation wait stack-create-complete --stack-name $STACK_NAME
      #     set +x
      #     WEBAPP_URL=$(aws cloudformation --region us-west-2 describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='ContentAnalyisSolution'].OutputValue" --output text)
      #     # Make the WEBAPP_URL available to the next workflow step
      #     echo "WEBAPP_URL=${WEBAPP_URL}" >> $GITHUB_ENV

      # - name: Clean build environment
      #   run: aws s3 rb s3://${DIST_OUTPUT_BUCKET}-${REGION} --force

      # - name: Get login credentials
      #   run: |
      #     # Iterate thru all the files in the s3://github-test-bot2 until you find the invitation email that references our stack
      #     NUM_EMAILS=$(aws s3 ls s3://github-test-bot2 | wc -l)
      #     for i in `seq 1 $NUM_EMAILS`; do
      #       INVITATION_EMAIL=$(aws s3api list-objects-v2 --bucket "github-test-bot2" --query 'reverse(sort_by(Contents, &LastModified))['$((i-1))'].Key' --output=text)
      #       # Make sure it belongs to our stack
      #       aws s3 cp s3://github-test-bot2/$INVITATION_EMAIL ./invitation_email --quiet
      #       STACK_NAME="pr${SHORT_SHA}"
      #       # Check to see if this invitation email is for the $STACK_NAME stack
      #       grep ":stack/${STACK_NAME}" ./invitation_email > /dev/null
      #       if [ $? -eq 0 ];
      #         then
      #         echo "Found invitation email in s3://github-test-bot2/$INVITATION_EMAIL"
      #         # we found the invitation email so quit looking
      #         break;
      #       fi;
      #     done;
      #     # Remove the invitation email from s3
      #     aws s3 rm s3://github-test-bot2/$INVITATION_EMAIL
      #     TEMP_PASSWORD=$(cat ./invitation_email | grep 'temporary password' | sed 's/.*password is \(.*\)<br>AWS.*/\1/')
      #     # Password may contain HTML entities, so decode them to characters
      #     TEMP_PASSWORD=$(echo $TEMP_PASSWORD | perl -MHTML::Entities -pe 'decode_entities($_);')
      #     # Make TEMP_PASSWORD available to the next workflow step
      #     echo "TEMP_PASSWORD=${TEMP_PASSWORD}" >> $GITHUB_ENV

      # - name: Start workflow
      #   run: |
      #     STACK_NAME="pr${SHORT_SHA}"
      #     # Get the workflow api endpoint
      #     MIE_STACK_NAME=$(aws cloudformation list-stacks --query 'StackSummaries[?starts_with(StackName,`'$STACK_NAME'-MieStack`) && StackStatus==`CREATE_COMPLETE`].StackName' --output json --region $REGION | grep MieStack | cut -f 2 -d '"' | tail -n 1)
      #     WORKFLOW_API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name "$MIE_STACK_NAME" --region $REGION --query "Stacks[0].Outputs[?OutputKey=='WorkflowApiEndpoint'].OutputValue" --output text)
      #     DATAPLANE_BUCKET=$(aws cloudformation describe-stacks --stack-name "$MIE_STACK_NAME" --region $REGION --query "Stacks[0].Outputs[?OutputKey=='DataplaneBucket'].OutputValue" --output text)
      #     # Upload a test video file
      #     wget -q https://techmkt-videoarchive.s3-us-west-2.amazonaws.com/amazon_studios/sizzle_reels/Amazon+TCA+2019+Series+Sizzle.mp4 -O AmazonVideoSizzle2019.mp4
      #     aws s3 cp AmazonVideoSizzle2019.mp4 s3://${DATAPLANE_BUCKET}
      #     # Install an IAM enabled HTTP client
      #     pip install awscurl

      #     #
      #     # Uncomment to enable CasImageWorkflow:
      #     #
      #     # #####################################
      #     # ###### TEST CasImageWorkflow  #######
      #     # #####################################
      #     # # TODO: upload TEST_IMAGE.png image file to dataplane bucket
      #     # # Get workflow configuration
      #     # WORKFLOW_NAME=CasImageWorkflow
      #     # # Disable faceSearch
      #     # WORKFLOW_CONFIGURATION=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/$WORKFLOW_NAME | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Stages.RekognitionStage.Configuration' --compact-output)
      #     # WORKFLOW_CONFIGURATION=$(echo $WORKFLOW_CONFIGURATION | sed -e 's/"faceSearchImage":{"MediaType":"Image","Enabled":true}/"faceSearchImage":{"MediaType":"Image","Enabled":false}/')
      #     # WORKFLOW_CONFIGURATION='{"RekognitionStage":'$WORKFLOW_CONFIGURATION'}'
      #     # # Execute workflow
      #     # awscurl -X POST --region us-west-2 --data '{"Name":"CasImageWorkflow", "Configuration":'$WORKFLOW_CONFIGURATION', "Input":{"Media":{"Image":{"S3Bucket": "'${DATAPLANE_BUCKET}'", "S3Key":"TEST_IMAGE.png"}}}}' ${WORKFLOW_API_ENDPOINT}workflow/execution > curl.txt

      #     # # Wait until the workflow is done
      #     # WORKFLOW_ID=$(cat curl.txt | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Id' --raw-output)
      #     # WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output)
      #     # while [ "$WORKFLOW_STATUS" = "Started" ] || [ "$WORKFLOW_STATUS" = "Queued" ]; do sleep 1; WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output); echo $WORKFLOW_STATUS; done

      #     ####################################
      #     ###### TEST CasVideoWorkflow #######
      #     ####################################
      #     WORKFLOW_NAME=CasVideoWorkflow
      #     # Disable faceSearch and GenericDataLookup operator
      #     WORKFLOW_CONFIGURATION=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/$WORKFLOW_NAME | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Stages.defaultVideoStage.Configuration' --compact-output)
      #     WORKFLOW_CONFIGURATION=$(echo $WORKFLOW_CONFIGURATION | sed -e 's/"faceSearch":{"MediaType":"Video","Enabled":true}/"faceSearch":{"MediaType":"Video","Enabled":false}/')
      #     WORKFLOW_CONFIGURATION=$(echo $WORKFLOW_CONFIGURATION | sed -e 's/"GenericDataLookup":{"MediaType":"Video","Enabled":true}/"GenericDataLookup":{"MediaType":"Video","Enabled":false}/')
      #     WORKFLOW_CONFIGURATION='{"defaultVideoStage":'$WORKFLOW_CONFIGURATION'}'
      #     echo "WORKFLOW_CONFIGURATION:"
      #     echo $WORKFLOW_CONFIGURATION
      #     echo "Starting CasVideoWorkflow"
      #     set -x
      #     # Execute workflow
      #     awscurl -X POST --region us-west-2 --data '{"Name":"CasVideoWorkflow", "Configuration":'$WORKFLOW_CONFIGURATION', "Input":{"Media":{"Video":{"S3Bucket": "'${DATAPLANE_BUCKET}'", "S3Key":"AmazonVideoSizzle2019.mp4"}}}}' ${WORKFLOW_API_ENDPOINT}workflow/execution > curl.txt
      #     set +x
      #     # Wait until the workflow is done
      #     WORKFLOW_ID=$(cat curl.txt | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Id' --raw-output)
      #     echo "WORKFLOW_ID: $WORKFLOW_ID"
      #     WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output)
      #     echo "Waiting for workflow to complete..."
      #     while [ "$WORKFLOW_STATUS" = "Started" ] || [ "$WORKFLOW_STATUS" = "Queued" ]; do sleep 10; WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output); echo -n '.'; done
      #     echo -e "\n$WORKFLOW_STATUS"

      # - name: Start puppeteer
      #   # The checkout action above changes the work dir in a way that breaks
      #   # the docker commands in this action, so we need to specify work dir
      #   # explicitly here.
      #   # TODO: after moving the aws-media-insights repo to aws-content-analysis, then remove the if statement at the beginning:
      #   working-directory: /home/runner/work/
      #   run: |
      #     if [ -d ./aws-media-insights/ ]; then
      #       cp -R ./aws-media-insights/aws-media-insights/source/website/test .
      #     elif [ -d ./aws-content-analysis/ ]; then
      #       cp -R ./aws-content-analysis/aws-content-analysis/source/website/test .
      #     else
      #       echo "ERROR: Cannot find test files"
      #       exit 1
      #     fi
      #     cd test/
      #     echo "Building puppeteer tests"
      #     npm init -y
      #     npm i puppeteer --quiet
      #     docker build --tag=cas-puppeteer:latest . --quiet
      #     echo "Running puppeteer tests"
      #     docker run --rm -v "$PWD":/usr/src/app -e WEBAPP_URL="${{ env.WEBAPP_URL }}" -e INVITATION_EMAIL_RECIPIENT="${{ secrets.INVITATION_EMAIL_RECIPIENT }}" -e TEMP_PASSWORD="${{ env.TEMP_PASSWORD }}" cas-puppeteer:latest

      # - name: Clean the test environment
      #   run: |
      #     # remove stack by default
      #     if [ -z ${{ github.event.inputs.remove_stack }} ] || [ ${{ github.event.inputs.remove_stack }} = "true" ]; then
      #       STACK_NAME="pr${SHORT_SHA}"
      #       echo "Removing $STACK_NAME"
      #       aws cloudformation delete-stack --stack-name $STACK_NAME --region $REGION
      #       echo "Waiting for stack to delete..."
      #       aws cloudformation wait stack-delete-complete --stack-name $STACK_NAME
      #       echo "Removing $STACK_NAME S3 buckets:"
      #       aws s3 ls | awk '{print $3}' | grep $STACK_NAME  | while read line; do
      #       echo "s3://$line";
      #       aws s3 rb s3://$line --force > /dev/null;
      #       done
      #     else
      #       echo "User requested not to delete stack."
      #     fi
